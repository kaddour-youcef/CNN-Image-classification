{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TPCVAI_1 (final).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMvGPjR0YD5p"
      },
      "source": [
        "#In this python programe we are developing a simple Keras Based CNN for image classification   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDyh1l-FgDpS"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras import optimizers\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\r\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\r\n",
        "from keras import callbacks\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JlFBIxFYD5s"
      },
      "source": [
        "#From scratch design a shallow CNN composed of a set of conv, pool and FC layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOz1HpKtovKP"
      },
      "source": [
        "img_width, img_height = 250, 250\r\n",
        "batch_size = 32\r\n",
        "samples_per_epoch = 3\r\n",
        "validation_steps = 300\r\n",
        "nb_filters1 = 32\r\n",
        "nb_filters2 = 64\r\n",
        "conv1_size = 3\r\n",
        "conv2_size = 2\r\n",
        "pool_size = 3\r\n",
        "classes_num = 2\r\n",
        "lr = 0.0004\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMcsl5Znojib"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Convolution2D(nb_filters1, conv1_size, conv1_size, padding=\"same\", input_shape=(img_width, img_height, 3)))\r\n",
        "model.add(Activation(\"relu\"))\r\n",
        "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\r\n",
        "\r\n",
        "model.add(Convolution2D(nb_filters2, conv2_size, conv2_size, padding= \"same\"))\r\n",
        "model.add(Activation(\"relu\"))\r\n",
        "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\r\n",
        "\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(256))\r\n",
        "model.add(Activation(\"relu\"))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(classes_num, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=optimizers.RMSprop(lr=lr),\r\n",
        "              metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpjcgF1aYD5t"
      },
      "source": [
        "#Train the model\n",
        "#Plot the error vs epoch curve\n",
        "#Report the accuracy on the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bYmfn8uAubZ"
      },
      "source": [
        "# for the purpuse of this example we are loading a series of images stored in a Google drive file \r\n",
        "#feel free ro change this part of the code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbMWndm8xkOZ",
        "outputId": "0a91b4f3-fc45-42ca-d2e6-832c92d9c8cb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "# Mount the google drive folder to colab:\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVgDoY6dgU2n",
        "outputId": "f410382d-748a-4b50-bbba-591810ba7bfa"
      },
      "source": [
        "train_data_path = 'gdrive/My Drive/Colab-Notebooks/Newdata/Train'\r\n",
        "validation_data_path = 'gdrive/My Drive/Colab-Notebooks/Newdata/Test'\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1. / 255,\r\n",
        "    shear_range=0.2,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    train_data_path,\r\n",
        "    target_size=(img_height, img_width),\r\n",
        "    batch_size=batch_size,\r\n",
        "    class_mode='categorical')\r\n",
        "\r\n",
        "validation_generator = test_datagen.flow_from_directory(\r\n",
        "    validation_data_path,\r\n",
        "    target_size=(img_height, img_width),\r\n",
        "    batch_size=batch_size,\r\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 82 images belonging to 2 classes.\n",
            "Found 82 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXSkp6g2LLn"
      },
      "source": [
        "DEV = False\r\n",
        "argvs = sys.argv\r\n",
        "argc = len(argvs)\r\n",
        "\r\n",
        "if argc > 1 and (argvs[1] == \"--development\" or argvs[1] == \"-d\"):\r\n",
        "  DEV = True\r\n",
        "\r\n",
        "if DEV:\r\n",
        "  epochs = 2\r\n",
        "else:\r\n",
        "  epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVSMtTDc1Gcz",
        "outputId": "64ce5ac7-a20f-464b-a395-b35ea836b5ac"
      },
      "source": [
        "log_dir = './tf-log/'\r\n",
        "tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\r\n",
        "cbks = [tb_cb]\r\n",
        "\r\n",
        "history = model.fit_generator(\r\n",
        "    train_generator,\r\n",
        "    samples_per_epoch,\r\n",
        "    epochs=epochs,\r\n",
        "    validation_data=validation_generator,\r\n",
        "    callbacks=cbks,\r\n",
        "    validation_steps=validation_steps)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "target_dir = 'gdrive/My Drive/Colab-Notebooks/models/'\r\n",
        "if not os.path.exists(target_dir):\r\n",
        "  os.mkdir(target_dir)\r\n",
        "model.save('gdrive/My Drive/Colab-Notebooks/models/model.h5')\r\n",
        "model.save_weights('gdrive/My Drive/Colab-Notebooks/models/weights.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9878WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "3/3 [==============================] - 2s 735ms/step - loss: 0.0227 - accuracy: 0.9878 - val_loss: 3.4241 - val_accuracy: 0.4390\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 0.0563 - accuracy: 0.9756\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.0408 - accuracy: 0.9756\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 1s 477ms/step - loss: 0.0474 - accuracy: 0.9756\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 1s 468ms/step - loss: 0.0254 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.0580 - accuracy: 0.9634\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.0395 - accuracy: 0.9878\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 1s 353ms/step - loss: 0.0519 - accuracy: 0.9878\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 0.0355 - accuracy: 0.9878\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.0383 - accuracy: 0.9756\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 1s 477ms/step - loss: 0.1061 - accuracy: 0.9634\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.1311 - accuracy: 0.9146\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 1s 452ms/step - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 1s 462ms/step - loss: 0.0255 - accuracy: 0.9878\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 1s 353ms/step - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 1s 474ms/step - loss: 0.0079 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8EjlwMGNEAs_",
        "outputId": "afe36578-7e2a-4221-8ed7-600abf7ba8f4"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkxqPTgYD5t"
      },
      "source": [
        "#Load in pre-trained weights from a network trained on a large dataset\r\n",
        "#Extract features from the train and test images using the pre-trained model\r\n",
        "#Classify the test images using a 1-nearst neighbour classifier\r\n",
        "#Load in pre-trained weights from a network trained on a large dataset\r\n",
        "#Freeze all the weights in the convolutional layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyjLNPpJHDFy"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P4YUOuNEy_t",
        "outputId": "79e59c8b-3fb4-4368-edf2-c0f4fb1aa396"
      },
      "source": [
        "model_path = 'gdrive/My Drive/Colab-Notebooks/models/model.h5'\r\n",
        "model_weights_path = 'gdrive/My Drive/Colab-Notebooks/models/weights.h5'\r\n",
        "test_path = 'gdrive/My Drive/Colab-Notebooks/Newdata/TEST-IM/'\r\n",
        "\r\n",
        "#Load the pre-trained models\r\n",
        "model = load_model(model_path)\r\n",
        "model.load_weights(model_weights_path)\r\n",
        "\r\n",
        "#Define image parameters\r\n",
        "img_width, img_height = 250, 250\r\n",
        "\r\n",
        "#Prediction Function\r\n",
        "def predict(file):\r\n",
        "  x = load_img(file, target_size=(img_width,img_height))\r\n",
        "  x = img_to_array(x)\r\n",
        "  x = np.expand_dims(x, axis=0)\r\n",
        "  array = model.predict(x)\r\n",
        "  result = array[0]\r\n",
        "  #print(result)\r\n",
        "  answer = np.argmax(result)\r\n",
        "  if answer == 1:\r\n",
        "    print(\"Predicted: Horse\")\r\n",
        "  elif answer == 0:\r\n",
        "    print (\"Predicted: Cow\")\r\n",
        "\r\n",
        "  return answer\r\n",
        "\r\n",
        "  #Walk the directory for every image\r\n",
        "for i, ret in enumerate(os.walk(test_path)):\r\n",
        "  for i, filename in enumerate(ret[2]):\r\n",
        "    if filename.startswith(\".\"):\r\n",
        "      continue\r\n",
        "    \r\n",
        "    print(ret[0] + '/' + filename)\r\n",
        "    result = predict(ret[0] + '/' + filename)\r\n",
        "    print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/Colab-Notebooks/Newdata/TEST-IM//horse5-090-202.png\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4c9a0be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Predicted: Horse\n",
            " \n",
            "gdrive/My Drive/Colab-Notebooks/Newdata/TEST-IM//horse5-090-338.png\n",
            "Predicted: Horse\n",
            " \n",
            "gdrive/My Drive/Colab-Notebooks/Newdata/TEST-IM//cow3-066-027.png\n",
            "Predicted: Cow\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}